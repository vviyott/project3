import streamlit as st
import json
import os
import tempfile
from datetime import datetime
from supabase import create_client
import numpy as np
# from openai import OpenAI  # ì´ ì¤„ ì œê±°
from sentence_transformers import SentenceTransformer  # ì¶”ê°€
import dotenv
import re

# í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
dotenv.load_dotenv()

# Supabase í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
supabase_url = os.environ.get("SUPABASE_URL")
supabase_key = os.environ.get("SUPABASE_KEY")
supabase = create_client(supabase_url, supabase_key)

# Sentence Transformer ëª¨ë¸ ì´ˆê¸°í™” (ë¬´ë£Œ)
@st.cache_resource
def load_embedding_model():
    """ì„ë² ë”© ëª¨ë¸ ë¡œë“œ (1536ì°¨ì›ìœ¼ë¡œ ë³€ê²½)"""
    # 1536ì°¨ì›ì„ ìƒì„±í•˜ëŠ” ë” í° ëª¨ë¸ ì‚¬ìš©
    return SentenceTransformer('sentence-transformers/all-mpnet-base-v2')

# ëª¨ë¸ ë¡œë“œ (ì´ ë¶€ë¶„ì´ ëˆ„ë½ë˜ì–´ ìˆì—ˆìŠµë‹ˆë‹¤!)
embedding_model = load_embedding_model()

def generate_embedding(text):
    """í…ìŠ¤íŠ¸ì—ì„œ ì„ë² ë”© ìƒì„± (1536ì°¨ì›)"""
    if not text or text.strip() == "":
        # ë¹ˆ í…ìŠ¤íŠ¸ì¸ ê²½ìš° ê¸°ë³¸ ì„ë² ë”© ë°˜í™˜
        return [0.0] * 1536  # 1536ì°¨ì›ìœ¼ë¡œ ìˆ˜ì •
    
    embedding = embedding_model.encode(text)
    # 1536ì°¨ì›ìœ¼ë¡œ íŒ¨ë”© ë˜ëŠ” í™•ì¥
    embedding_list = embedding.tolist()
    
    # 768ì°¨ì›ì„ 1536ì°¨ì›ìœ¼ë¡œ í™•ì¥ (0ìœ¼ë¡œ íŒ¨ë”©)
    if len(embedding_list) < 1536:
        embedding_list.extend([0.0] * (1536 - len(embedding_list)))
    
    return embedding_list[:1536]  # ì •í™•íˆ 1536ì°¨ì›ë§Œ ë°˜í™˜

def clean_html_tags(text):
    """HTML íƒœê·¸ ì œê±°"""
    if not text:
        return ""
    return re.sub(r'<.*?>', '', text)

def detect_naver_api_type(data):
    """ë„¤ì´ë²„ API ì‘ë‹µ íƒ€ì… ê°ì§€ (ë¸”ë¡œê·¸, ì‡¼í•‘, ë‰´ìŠ¤)"""
    if not isinstance(data, dict) or 'items' not in data:
        return "unknown"
    
    # ìƒ˜í”Œ ì•„ì´í…œ í™•ì¸
    if not data['items']:
        return "unknown"
    
    sample_item = data['items'][0]
    
    # íƒ€ì… ê°ì§€ ë¡œì§
    if 'bloggername' in sample_item:
        return "ë¸”ë¡œê·¸"
    elif 'productType' in sample_item or 'maker' in sample_item or 'mallName' in sample_item:
        return "ì‡¼í•‘"
    elif 'pubDate' in sample_item and ('articleId' in sample_item or 'originallink' in sample_item):
        return "ë‰´ìŠ¤"
    else:
        return "unknown"

def process_json_file(file_path, collection_name=None, source_type=None):
    """JSON íŒŒì¼ ì²˜ë¦¬ ë° Supabaseì— ì €ì¥"""
    # JSON íŒŒì¼ ë¡œë“œ
    with open(file_path, 'r', encoding='utf-8') as f:
        data = json.load(f)
    
    # ë„¤ì´ë²„ API ì‘ë‹µ êµ¬ì¡° í™•ì¸
    if isinstance(data, dict) and 'items' in data:
        # ë„¤ì´ë²„ API ì‘ë‹µ í˜•ì‹ì¸ ê²½ìš°
        items = data['items']
        
        # ì†ŒìŠ¤ íƒ€ì…ì´ ì§€ì •ë˜ì§€ ì•Šì€ ê²½ìš° ìë™ ê°ì§€
        if not source_type:
            source_type = detect_naver_api_type(data)
            st.info(f"ë°ì´í„° í˜•ì‹ì´ '{source_type}'ìœ¼ë¡œ ê°ì§€ë˜ì—ˆìŠµë‹ˆë‹¤.")
    else:
        # ì§ì ‘ JSON ë°°ì—´ì¸ ê²½ìš°
        items = data
    
    # ì»¬ë ‰ì…˜ ì´ë¦„ ìƒì„±
    if not collection_name:
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        collection_name = f'{source_type}_{timestamp}'
    
    # ì²˜ë¦¬ëœ ë¬¸ì„œ ìˆ˜ ì¹´ìš´íŠ¸
    doc_count = 0
    
    # ê° í•­ëª© ì²˜ë¦¬
    for i, item in enumerate(items):
        # ì†ŒìŠ¤ íƒ€ì…ë³„ë¡œ ë‹¤ë¥¸ í•„ë“œ ì²˜ë¦¬
        if source_type == "ë¸”ë¡œê·¸":
            title = clean_html_tags(item.get('title', ''))
            content = clean_html_tags(item.get('description', ''))
            full_content = title + " " + content
            
            metadata = {
                "title": title,
                "collection": source_type,
                "collected_at": datetime.now().isoformat(),
                "url": item.get('link', ''),
                "date": item.get('postdate', ''),
                "bloggername": item.get('bloggername', ''),
                "bloggerlink": item.get('bloggerlink', '')
            }
            
        elif source_type == "ì‡¼í•‘":
            title = clean_html_tags(item.get('title', ''))
            content = clean_html_tags(item.get('description', item.get('category3', '')))
            full_content = title + " " + content
            
            # ê°€ê²© ì •ë³´ ìˆ«ìë¡œ ë³€í™˜
            price = item.get('lprice', '')
            try:
                price = int(price)
            except (ValueError, TypeError):
                price = None
                
            metadata = {
                "title": title,
                "collection": source_type,
                "collected_at": datetime.now().isoformat(),
                "url": item.get('link', ''),
                "price": price,
                "maker": item.get('maker', ''),
                "brand": item.get('brand', ''),
                "mallName": item.get('mallName', ''),
                "productId": item.get('productId', ''),
                "productType": item.get('productType', '')
            }
            
        elif source_type == "ë‰´ìŠ¤":
            title = clean_html_tags(item.get('title', ''))
            content = clean_html_tags(item.get('description', ''))
            full_content = title + " " + content
            
            metadata = {
                "title": title,
                "collection": source_type,
                "collected_at": datetime.now().isoformat(),
                "url": item.get('link', item.get('originallink', '')),
                "date": item.get('pubDate', ''),
                "publisher": item.get('publisher', '')
            }
            
        else:
            # ê¸°ë³¸ ì²˜ë¦¬ (íƒ€ì…ì´ ë¶ˆë¶„ëª…í•œ ê²½ìš°)
            title = clean_html_tags(item.get('title', ''))
            content = clean_html_tags(item.get('description', item.get('content', '')))
            full_content = title + " " + content
            
            metadata = {
                "title": title,
                "collection": source_type if source_type else "general",
                "collected_at": datetime.now().isoformat()
            }
            
            # ê³µí†µ í•„ë“œ ì¶”ê°€
            if 'link' in item:
                metadata['url'] = item['link']
            
        # ì„ë² ë”© ìƒì„± (ë¬´ë£Œ ëª¨ë¸ ì‚¬ìš©)
        embedding = generate_embedding(full_content)
        
        # Supabaseì— ë°ì´í„° ì‚½ì…
        data = {
            'content': full_content,
            'embedding': embedding,
            'metadata': metadata
        }
        
        supabase.table('documents').insert(data).execute()
        doc_count += 1
    
    return collection_name, doc_count, source_type

# Streamlit ì•± UI
st.title("ë„¤ì´ë²„ JSON íŒŒì¼ì„ Supabaseì— ì €ì¥í•˜ê¸°")

# ëª¨ë¸ ì •ë³´ í‘œì‹œ (ëª¨ë¸ëª… ìˆ˜ì •)
st.sidebar.info("ğŸ†“ ë¬´ë£Œ ì„ë² ë”© ëª¨ë¸ ì‚¬ìš© ì¤‘: all-mpnet-base-v2")

uploaded_file = st.file_uploader("JSON íŒŒì¼ ì—…ë¡œë“œ", type=['json'])

if uploaded_file is not None:
    # íŒŒì¼ ì €ì¥
    with tempfile.NamedTemporaryFile(delete=False, suffix='.json') as tmp_file:
        tmp_file.write(uploaded_file.getvalue())
        tmp_file_path = tmp_file.name
    
    # íƒ€ì… ì„ íƒ
    source_type = st.radio(
        "ë°ì´í„° ì†ŒìŠ¤ íƒ€ì… ì„ íƒ (ìë™ ê°ì§€í•˜ë ¤ë©´ 'ìë™ ê°ì§€' ì„ íƒ)",
        ['ìë™ ê°ì§€', 'ë¸”ë¡œê·¸', 'ì‡¼í•‘', 'ë‰´ìŠ¤']
    )
    
    # ìë™ ê°ì§€ì¸ ê²½ìš° Noneìœ¼ë¡œ ì„¤ì •
    if source_type == 'ìë™ ê°ì§€':
        source_type = None
    
    # ì»¬ë ‰ì…˜ ì´ë¦„ ì…ë ¥
    collection_name = st.text_input("ì»¬ë ‰ì…˜ ì´ë¦„ (ì…ë ¥í•˜ì§€ ì•Šìœ¼ë©´ ìë™ ìƒì„±ë©ë‹ˆë‹¤)")
    
    if st.button("Supabaseì— ì €ì¥"):
        with st.spinner("ë°ì´í„° ì²˜ë¦¬ ì¤‘..."):
            try:
                collection_name, doc_count, detected_type = process_json_file(
                    tmp_file_path, 
                    collection_name, 
                    source_type
                )
                
                st.success(f"ì„±ê³µì ìœ¼ë¡œ {doc_count}ê°œì˜ ë¬¸ì„œê°€ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤!")
                st.write(f"ì»¬ë ‰ì…˜ ì´ë¦„: {collection_name}")
                st.write(f"ë°ì´í„° íƒ€ì…: {detected_type}")
                
                # ë°ì´í„°ë² ì´ìŠ¤ ìƒíƒœ í‘œì‹œ
                try:
                    result = supabase.table('documents').select('id', count='exact').execute()
                    doc_count_total = result.count if hasattr(result, 'count') else len(result.data)
                    st.write(f"ë°ì´í„°ë² ì´ìŠ¤ ì´ ë¬¸ì„œ ìˆ˜: {doc_count_total}ê°œ")
                except Exception as e:
                    st.warning(f"ë°ì´í„°ë² ì´ìŠ¤ ìƒíƒœ í™•ì¸ ì¤‘ ì˜¤ë¥˜: {str(e)}")
                
            except Exception as e:
                st.error(f"ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
            finally:
                # ì„ì‹œ íŒŒì¼ ì‚­ì œ
                os.unlink(tmp_file_path)
